{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CzuxRTYVKgPy",
    "outputId": "b4c1d81a-9d36-41cd-aa92-a5251f19d1f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
      "/content/gdrive/MyDrive/wj-cnn\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd '/content/gdrive/MyDrive/wj-cnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0PtGiu9fKmoU",
    "outputId": "c244fc74-8bfd-42e5-8824-be21cb9c349e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "if_use_gpu=torch.cuda.is_available()\n",
    "print(\"GPU:\",if_use_gpu)\n",
    "def seed_torch(seed=1029):\n",
    "\trandom.seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)  # CPU\n",
    "\ttorch.cuda.manual_seed(seed)  # GPU\n",
    "\ttorch.cuda.manual_seed_all(seed)  # Multi-GPU.\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed=1029\n",
    "seed_torch(seed)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kA4kE5tjPQDt"
   },
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdYVJIBoPSdw"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, img_file, label_file, transform=None, idx = None):\n",
    "        self.data = pickle.load( open( img_file, 'rb' ), encoding='bytes')\n",
    "        self.targets = np.genfromtxt(label_file, delimiter=',', skip_header=1)[:,1:]\n",
    "        if idx is not None:\n",
    "          self.targets = self.targets[idx]\n",
    "          self.data = self.data[idx]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], int(self.targets[index]) - 5  \n",
    "        img = Image.fromarray(img.astype('uint8'), mode='L')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "           img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLmlfFPSPWHo"
   },
   "outputs": [],
   "source": [
    "def data_loader(batchsize):\n",
    "  dataset = MyDataset('./Train.pkl', './TrainLabels.csv',transform=img_transform, idx=None)\n",
    "  train_dataset, test_dataset = train_test_split(dataset, test_size=0.1)\n",
    "\n",
    "  batch_size = batchsize  \n",
    "  def _init_fn(worker_id):\n",
    "      np.random.seed(int(seed)+worker_id)\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True, worker_init_fn=_init_fn)  \n",
    "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True, worker_init_fn=_init_fn)  \n",
    "\n",
    "  return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRT9yyP8PfaV"
   },
   "outputs": [],
   "source": [
    "## Six-layer network\n",
    "\n",
    "class Net(nn.Module):\n",
    "    # This part defines the layers\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()  # 1*128*128\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding = 2) \n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding = 2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding = 1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16384, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 9)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(F.max_pool2d(self.bn1(self.conv1(x)), 2))  \n",
    "        x = F.relu(F.max_pool2d(self.bn2(self.conv2(x)), 2))  \n",
    "        x = F.relu(F.max_pool2d(self.bn3(self.conv3(x)), 2))  \n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdLMpvHLB4wt"
   },
   "outputs": [],
   "source": [
    "## LeNet-5\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    # This part defines the layers\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()  \n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.fc1 = nn.Linear(13456, 120, bias = True)\n",
    "        self.fc2 = nn.Linear(120, 84, bias = True)\n",
    "        self.fc3 = nn.Linear(84, 9, bias = True)  \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(F.max_pool2d(self.bn1(self.conv1(x)), 2))  \n",
    "        x = F.relu(F.max_pool2d(self.bn2(self.conv2(x)), 2))  \n",
    " \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "LeNet_5 = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-AkCpOaDUwF"
   },
   "outputs": [],
   "source": [
    "## AlexNet\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2), # kernel_size, stride\n",
    "            nn.Conv2d(96, 256, 5, 1, 2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2),\n",
    "            nn.Conv2d(256, 384, 3, 1, 1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, 3, 1, 1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1024, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 9)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "        output = self.fc(feature.view(img.shape[0], -1))\n",
    "        return F.log_softmax(output)\n",
    "\n",
    "\n",
    "AlexNet_8 = AlexNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UsJJkVNrZ0U3"
   },
   "outputs": [],
   "source": [
    "# VGG-16\n",
    "\n",
    "class VGG1(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(VGG1, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 64, 3, 1, 1), # in_channels, out_channels, kernel_size, stride, padding\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1), # in_channels, out_channels, kernel_size, stride, padding\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),            \n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),            \n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),            \n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Block 5\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(8192, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 9),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "        output = self.fc(feature.view(img.shape[0], -1))\n",
    "        return F.log_softmax(output)\n",
    "\n",
    "VGG_16 = VGG1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_OHDwM3hA3B"
   },
   "outputs": [],
   "source": [
    "# VGG-19\n",
    "\n",
    "class VGG2(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(VGG2, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 64, 3, 1, 1), # in_channels, out_channels, kernel_size, stride, padding\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1), # in_channels, out_channels, kernel_size, stride, padding\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),            \n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),            \n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),    \n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),           \n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Block 5\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(8192, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 9),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "        output = self.fc(feature.view(img.shape[0], -1))\n",
    "        return F.log_softmax(output)\n",
    "\n",
    "VGG_19 = VGG2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xyvvze657Ryb"
   },
   "outputs": [],
   "source": [
    "## NiNet\n",
    "\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "\n",
    "class NiNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NiNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "                # NIN_1\n",
    "                nn.Conv2d(1, 96, 11, 4),\n",
    "                nn.BatchNorm2d(96),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(96, 96, 1),\n",
    "                nn.BatchNorm2d(96),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(96, 96, 1),\n",
    "                nn.BatchNorm2d(96),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "                # NIN_2\n",
    "                nn.Conv2d(96, 256, 5, 1, 2),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(256, 256, 1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(256, 256, 1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "                # NIN_3\n",
    "                nn.Conv2d(256, 384, 3, 1, 1),\n",
    "                nn.BatchNorm2d(384),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(384, 384, 1),\n",
    "                nn.BatchNorm2d(384),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(384, 384, 1),\n",
    "                nn.BatchNorm2d(384),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "\n",
    "                # NIN_4\n",
    "                nn.Conv2d(384, 9, 3, 1, 1),\n",
    "                nn.BatchNorm2d(9),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(9, 9, 1),\n",
    "                nn.BatchNorm2d(9),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(9, 9, 1),\n",
    "                nn.BatchNorm2d(9),\n",
    "                nn.ReLU(),    \n",
    "\n",
    "                GlobalAvgPool2d() \n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "        output = feature.view(img.shape[0], -1)\n",
    "        return F.log_softmax(output)\n",
    "\n",
    "NiN = NiNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3sdJZ4C5fjW"
   },
   "outputs": [],
   "source": [
    "## ResNet-18\n",
    "\n",
    "class ResNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(ResNet1, self).__init__()\n",
    "\n",
    "        # Block 1  \n",
    "        self.conv1 = nn.Conv2d(1, 64, 7, 2, 3) # in_channels, out_channels, kernel_size, stride, padding  \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Block 2 Resnet_1\n",
    "        self.conv2_1 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(64)\n",
    "        self.conv2_2 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(64)   \n",
    "\n",
    "        # Block 3 Resnet_2\n",
    "        self.conv3_1 = nn.Conv2d(64, 128, 3, 1, 1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(128)\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, 3, 1, 1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(128)\n",
    "        self.conv3_3 = nn.Conv2d(64, 128, 1, 1)\n",
    "        self.bn3_3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Block 4 Resnet_3\n",
    "        self.conv4_1 = nn.Conv2d(128, 256, 3, 1, 1) \n",
    "        self.bn4_1 = nn.BatchNorm2d(256)\n",
    "        self.conv4_2 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "        self.bn4_2 = nn.BatchNorm2d(256)\n",
    "        self.conv4_3 = nn.Conv2d(128, 256, 1, 1) \n",
    "        self.bn4_3 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Block 5 Resnet_4\n",
    "        self.conv5_1 = nn.Conv2d(256, 512, 3, 1, 1) # 512*32*32\n",
    "        self.bn5_1 = nn.BatchNorm2d(512)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "        self.bn5_2 = nn.BatchNorm2d(512)  \n",
    "        self.conv5_3 = nn.Conv2d(256, 512, 1, 1) \n",
    "        self.bn5_3 = nn.BatchNorm2d(512) \n",
    "\n",
    "        self.fc = nn.Linear(512, 9)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Block 1\n",
    "        X = F.max_pool2d(F.relu(self.bn1(self.conv1(X))), kernel_size=3, stride=2, padding=1) \n",
    "\n",
    "        # Block 2 Resnet_1\n",
    "        Y = F.relu(self.bn2_1(self.conv2_1(X)))\n",
    "        Y = self.bn2_2(self.conv2_2(Y))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 3 Resnet_2\n",
    "        Y = F.relu(self.bn3_1(self.conv3_1(X)))\n",
    "        Y = self.bn3_2(self.conv3_2(Y))\n",
    "        X = self.bn3_3(self.conv3_3(X))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 4 Resnet_3\n",
    "        Y = F.relu(self.bn4_1(self.conv4_1(X)))\n",
    "        Y = self.bn4_2(self.conv4_2(Y))\n",
    "        X = self.bn4_3(self.conv4_3(X))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 5 Resnet_4\n",
    "        Y = F.relu(self.bn5_1(self.conv5_1(X)))\n",
    "        Y = self.bn5_2(self.conv5_2(Y))\n",
    "        X = self.bn5_3(self.conv5_3(X))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        X = F.avg_pool2d(X, kernel_size=X.size()[2:])\n",
    "\n",
    "        X = X.view(X.size(0), -1)\n",
    "\n",
    "        output = self.fc(X)\n",
    "        return F.log_softmax(output)\n",
    "\n",
    "ResNet_18 = ResNet1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bH-IGmLG74ZS"
   },
   "outputs": [],
   "source": [
    "## ResNet-34\n",
    "\n",
    "class ResNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(ResNet2, self).__init__()\n",
    "\n",
    "        # Block 1   # 128*128\n",
    "        self.conv1 = nn.Conv2d(1, 64, 7, 2, 3) # in_channels, out_channels, kernel_size, stride, padding  # 64*64*64\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Block 2 Resnet_1-1\n",
    "        self.conv2_1_1 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "        self.bn2_1_1 = nn.BatchNorm2d(64)\n",
    "        self.conv2_2_1 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "        self.bn2_2_1 = nn.BatchNorm2d(64)    \n",
    "\n",
    "        # Block 2 Resnet_1-2\n",
    "        self.conv2_1_2 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "        self.bn2_1_2 = nn.BatchNorm2d(64)\n",
    "        self.conv2_2_2 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "        self.bn2_2_2 = nn.BatchNorm2d(64)   \n",
    "\n",
    "        # Block 2 Resnet_1-3\n",
    "        self.conv2_1_3 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "        self.bn2_1_3 = nn.BatchNorm2d(64)\n",
    "        self.conv2_2_3 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "        self.bn2_2_3 = nn.BatchNorm2d(64) \n",
    "\n",
    "        # Block 3 Resnet_2_1\n",
    "        self.conv3_1_1 = nn.Conv2d(64, 128, 3, 1, 1)\n",
    "        self.bn3_1_1 = nn.BatchNorm2d(128)\n",
    "        self.conv3_2_1 = nn.Conv2d(128, 128, 3, 1, 1)\n",
    "        self.bn3_2_1 = nn.BatchNorm2d(128)\n",
    "        self.conv3_3_1 = nn.Conv2d(64, 128, 1, 1)\n",
    "        self.bn3_3_1 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Block 3 Resnet_2_2\n",
    "        self.conv3_1_2 = nn.Conv2d(128, 128, 3, 1, 1)\n",
    "        self.bn3_1_2 = nn.BatchNorm2d(128)\n",
    "        self.conv3_2_2 = nn.Conv2d(128, 128, 3, 1, 1)\n",
    "        self.bn3_2_2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Block 3 Resnet_2_3\n",
    "        self.conv3_1_3 = nn.Conv2d(128, 128, 3, 1, 1)\n",
    "        self.bn3_1_3 = nn.BatchNorm2d(128)\n",
    "        self.conv3_2_3 = nn.Conv2d(128, 128, 3, 1, 1)\n",
    "        self.bn3_2_3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Block 3 Resnet_2_4\n",
    "        self.conv3_1_4 = nn.Conv2d(128, 128, 3, 1, 1)\n",
    "        self.bn3_1_4 = nn.BatchNorm2d(128)\n",
    "        self.conv3_2_4 = nn.Conv2d(128, 128, 3, 1, 1)\n",
    "        self.bn3_2_4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Block 4 Resnet_3_1\n",
    "        self.conv4_1_1 = nn.Conv2d(128, 256, 3, 1, 1) \n",
    "        self.bn4_1_1 = nn.BatchNorm2d(256)\n",
    "        self.conv4_2_1 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "        self.bn4_2_1 = nn.BatchNorm2d(256)\n",
    "        self.conv4_3_1 = nn.Conv2d(128, 256, 1, 1) \n",
    "        self.bn4_3_1 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Block 4 Resnet_3_2\n",
    "        self.conv4_1_2 = nn.Conv2d(256, 256, 3, 1, 1) \n",
    "        self.bn4_1_2 = nn.BatchNorm2d(256)\n",
    "        self.conv4_2_2 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "        self.bn4_2_2 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Block 4 Resnet_3_3\n",
    "        self.conv4_1_3 = nn.Conv2d(256, 256, 3, 1, 1) \n",
    "        self.bn4_1_3 = nn.BatchNorm2d(256)\n",
    "        self.conv4_2_3 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "        self.bn4_2_3 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Block 4 Resnet_3_4\n",
    "        self.conv4_1_4 = nn.Conv2d(256, 256, 3, 1, 1) \n",
    "        self.bn4_1_4 = nn.BatchNorm2d(256)\n",
    "        self.conv4_2_4 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "        self.bn4_2_4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Block 4 Resnet_3_5\n",
    "        self.conv4_1_5 = nn.Conv2d(256, 256, 3, 1, 1) \n",
    "        self.bn4_1_5 = nn.BatchNorm2d(256)\n",
    "        self.conv4_2_5 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "        self.bn4_2_5 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Block 4 Resnet_3_6\n",
    "        self.conv4_1_6 = nn.Conv2d(256, 256, 3, 1, 1) \n",
    "        self.bn4_1_6 = nn.BatchNorm2d(256)\n",
    "        self.conv4_2_6 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "        self.bn4_2_6 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Block 5 Resnet_4_1\n",
    "        self.conv5_1_1 = nn.Conv2d(256, 512, 3, 1, 1) # 512*32*32\n",
    "        self.bn5_1_1 = nn.BatchNorm2d(512)\n",
    "        self.conv5_2_1 = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "        self.bn5_2_1 = nn.BatchNorm2d(512)  \n",
    "        self.conv5_3_1 = nn.Conv2d(256, 512, 1, 1) \n",
    "        self.bn5_3_1 = nn.BatchNorm2d(512) \n",
    "\n",
    "        # Block 5 Resnet_4_2\n",
    "        self.conv5_1_2 = nn.Conv2d(512, 512, 3, 1, 1) \n",
    "        self.bn5_1_2 = nn.BatchNorm2d(512)\n",
    "        self.conv5_2_2 = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "        self.bn5_2_2 = nn.BatchNorm2d(512) \n",
    "\n",
    "        # Block 5 Resnet_4_3\n",
    "        self.conv5_1_3 = nn.Conv2d(512, 512, 3, 1, 1) \n",
    "        self.bn5_1_3 = nn.BatchNorm2d(512)\n",
    "        self.conv5_2_3 = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "        self.bn5_2_3 = nn.BatchNorm2d(512) \n",
    "\n",
    "        self.fc = nn.Linear(512, 9)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Block 1\n",
    "        X = F.max_pool2d(F.relu(self.bn1(self.conv1(X))), kernel_size=3, stride=2, padding=1) \n",
    "\n",
    "        # Block 2 Resnet_1_1\n",
    "        Y = F.relu(self.bn2_1_1(self.conv2_1_1(X)))\n",
    "        Y = self.bn2_2_1(self.conv2_2_1(Y))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 2 Resnet_1_2\n",
    "        Y = F.relu(self.bn2_1_2(self.conv2_1_2(X)))\n",
    "        Y = self.bn2_2_2(self.conv2_2_2(Y))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 2 Resnet_1_3\n",
    "        Y = F.relu(self.bn2_1_3(self.conv2_1_3(X)))\n",
    "        Y = self.bn2_2_3(self.conv2_2_3(Y))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 3 Resnet_2_1\n",
    "        Y = F.relu(self.bn3_1_1(self.conv3_1_1(X)))\n",
    "        Y = self.bn3_2_1(self.conv3_2_1(Y))\n",
    "        X = self.bn3_3_1(self.conv3_3_1(X))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 3 Resnet_2_2\n",
    "        Y = F.relu(self.bn3_1_2(self.conv3_1_2(X)))\n",
    "        Y = self.bn3_2_2(self.conv3_2_2(Y))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 3 Resnet_2_3\n",
    "        Y = F.relu(self.bn3_1_3(self.conv3_1_3(X)))\n",
    "        Y = self.bn3_2_3(self.conv3_2_3(Y))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 3 Resnet_2_4\n",
    "        Y = F.relu(self.bn3_1_4(self.conv3_1_4(X)))\n",
    "        Y = self.bn3_2_4(self.conv3_2_4(Y))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 4 Resnet_3_1\n",
    "        Y = F.relu(self.bn4_1_1(self.conv4_1_1(X)))\n",
    "        Y = self.bn4_2_1(self.conv4_2_1(Y))\n",
    "        X = self.bn4_3_1(self.conv4_3_1(X))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 4 Resnet_3_2\n",
    "        Y = F.relu(self.bn4_1_2(self.conv4_1_2(X)))\n",
    "        Y = self.bn4_2_2(self.conv4_2_2(Y))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 4 Resnet_3_3\n",
    "        Y = F.relu(self.bn4_1_3(self.conv4_1_3(X)))\n",
    "        Y = self.bn4_2_3(self.conv4_2_3(Y))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 4 Resnet_3_4\n",
    "        Y = F.relu(self.bn4_1_4(self.conv4_1_4(X)))\n",
    "        Y = self.bn4_2_4(self.conv4_2_4(Y))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 4 Resnet_3_5\n",
    "        Y = F.relu(self.bn4_1_5(self.conv4_1_5(X)))\n",
    "        Y = self.bn4_2_5(self.conv4_2_5(Y))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 4 Resnet_3_6\n",
    "        Y = F.relu(self.bn4_1_6(self.conv4_1_6(X)))\n",
    "        Y = self.bn4_2_6(self.conv4_2_6(Y))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 5 Resnet_4_1\n",
    "        Y = F.relu(self.bn5_1_1(self.conv5_1_1(X))) \n",
    "        Y = self.bn5_2_1(self.conv5_2_1(Y))\n",
    "        X = self.bn5_3_1(self.conv5_3_1(X))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 5 Resnet_4_2\n",
    "        Y = F.relu(self.bn5_1_2(self.conv5_1_2(X)))\n",
    "        Y = self.bn5_2_2(self.conv5_2_2(Y))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        # Block 5 Resnet_4_3\n",
    "        Y = F.relu(self.bn5_1_3(self.conv5_1_3(X)))\n",
    "        Y = self.bn5_2_3(self.conv5_2_3(Y))\n",
    "        X = F.relu(Y + X)\n",
    "\n",
    "        X = F.avg_pool2d(X, kernel_size=X.size()[2:])\n",
    "\n",
    "\n",
    "        # This layer is an imaginary one. It simply states that we should see each member of x\n",
    "        X = X.view(X.size(0), -1)\n",
    "\n",
    "        output = self.fc(X)\n",
    "        return F.log_softmax(output)\n",
    "\n",
    "ResNet_34 = ResNet2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsWM1ChUOBGm"
   },
   "outputs": [],
   "source": [
    "def net_and_optm(model, learning_rate):\n",
    "  if if_use_gpu:\n",
    "      network = model.cuda()\n",
    "  else:\n",
    "      network = model\n",
    "\n",
    "  optimizer_sgd = optim.SGD(network.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
    "\n",
    "  return network, optimizer_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t08oK7B-PjXA"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_acc = []\n",
    "\n",
    "def train(network, epoch, train_loader, test_loader, optimizer_sgd, batchsize):\n",
    "  network.train()\n",
    "  correct = 0\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    if if_use_gpu:\n",
    "      data=data.cuda()\n",
    "      target=target.cuda()\n",
    "\n",
    "    optimizer=optimizer_sgd\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    if if_use_gpu:\n",
    "      loss = F.nll_loss(output, target).cuda() # negative log liklhood loss\n",
    "    else:\n",
    "      loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "      \n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "    if batch_idx % 20 == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.cpu().item()))\n",
    "      print(\"Accuracy: {}/{} ({:.0f}%)\".format(correct, len(train_loader.dataset),\n",
    "         100. * correct / len(train_loader.dataset)))\n",
    "      train_losses.append(loss.cpu().item())\n",
    "      train_counter.append(\n",
    "        (batch_idx * batchsize) + ((epoch)*len(train_loader.dataset)))\n",
    "      torch.save(network.state_dict(), './model.pth')\n",
    "      torch.save(optimizer.state_dict(), './optimizer.pth')\n",
    "\n",
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      if if_use_gpu:\n",
    "        data=data.cuda()\n",
    "        target=target.cuda()\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).cpu().item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  correct_pecent=correct / len(test_loader.dataset)\n",
    "  test_acc.append(correct_pecent.cpu())\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "      test_loss, correct, len(test_loader.dataset),\n",
    "      100. * correct_pecent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xy4OQJgrwBPC"
   },
   "outputs": [],
   "source": [
    "def train_epochs(network, num_epoch, train_loader, test_loader, optimizer_sgd, batchsize):\n",
    "  for epoch in range(num_epoch):\n",
    "    start = time.time()\n",
    "    train(network, epoch, train_loader, test_loader, optimizer_sgd, batchsize)\n",
    "    duration = time.time() - start \n",
    "    print('Training duation: %.4f'%duration)\n",
    "    test(network, test_loader)\n",
    "\n",
    "  # Training loss\n",
    "  fig = plt.figure() \n",
    "  plt.plot(train_counter, train_losses, color='blue')\n",
    "  plt.legend(['Train Loss'], loc='upper right')\n",
    "  plt.xlabel('number of training examples seen')\n",
    "  plt.ylabel('negative log likelihood loss')\n",
    "  import seaborn as sns\n",
    "  import pandas as pd\n",
    "  sns.set()\n",
    "  sns.lineplot(data=pd.DataFrame(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUiqC6IKzyO2"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_file, transform=None, idx = None):\n",
    "        self.data = pickle.load( open( img_file, 'rb' ), encoding='bytes')\n",
    "        if idx is not None:\n",
    "          self.data = self.data[idx]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return 10000\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index] \n",
    "        img = Image.fromarray(img.astype('uint8'), mode='L')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "          img = self.transform(img)\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyb5QK1Xw8ns"
   },
   "outputs": [],
   "source": [
    "# Prediction for test dataset\n",
    "def prediction(batchsize, network):\n",
    "  testdata = TestDataset('./Test.pkl',transform=img_transform, idx=None)\n",
    "  def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed)+worker_id)\n",
    "  testdata_loader = DataLoader(testdata, batch_size=batchsize, shuffle=False, num_workers=16, pin_memory=True, worker_init_fn=_init_fn)  \n",
    "\n",
    "  prediction = [1]\n",
    "  for data in testdata_loader:\n",
    "      if if_use_gpu:\n",
    "        data=data.cuda()\n",
    "      output = network(data)\n",
    "      pred = output.data.max(1, keepdim=True)[1].cpu().numpy()\n",
    "      prediction = np.vstack([prediction, pred])\n",
    "\n",
    "  Test_pred = prediction[1:,:] + 5\n",
    "\n",
    "  return Test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUDCYl_ruIWs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def main(model, batchsize, learning_rate, num_epoch):\n",
    "  train_loader, test_loader = data_loader(batchsize)\n",
    "  network, optimizer_sgd = net_and_optm(model, learning_rate)\n",
    "  train_epochs(network, num_epoch, train_loader, test_loader, optimizer_sgd, batchsize)\n",
    "  Test_pred = prediction(batchsize, network)\n",
    "  # Save to CSV\n",
    "  save = pd.DataFrame(Test_pred) \n",
    "  save.to_csv(\"Test_pred.csv\", header = ['class'], index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4r3kZFFy_a8"
   },
   "outputs": [],
   "source": [
    "main(model = net, batchsize = 64, learning_rate = 0.05, num_epoch = 40)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Mini_Project_3_revised_1206.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
